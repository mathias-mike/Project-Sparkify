{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sparkify Control Point"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# First things first, we have to create a redshift cluster for our project on AWS\r\n",
    "# Here, we'd be using IaC to proceed with the processes\r\n",
    "\r\n",
    "# importing boto3, AWS python SDK\r\n",
    "import boto3\r\n",
    "from botocore.exceptions import ClientError\r\n",
    "\r\n",
    "import configparser\r\n",
    "import json\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Extracting config variables \r\n",
    "config = configparser.ConfigParser()\r\n",
    "config.read_file(open('dwh.cfg'))\r\n",
    "\r\n",
    "KEY = config.get('USER', 'KEY')\r\n",
    "SECRET = config.get('USER', 'SECRET')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "DWH_ROLE_NAME = config.get('DWH', 'DWH_ROLE_NAME')\r\n",
    "DWH_DB_NAME = config.get('DWH', 'DWH_DB_NAME')\r\n",
    "DWH_CLUSTER_ID = config.get('DWH', 'DWH_CLUSTER_ID')\r\n",
    "DWH_NODE_TYPE = config.get('DWH', 'DWH_NODE_TYPE')\r\n",
    "DWH_USER_NAME = config.get('DWH', 'DWH_USER_NAME')\r\n",
    "DWH_USER_PASSWORD = config.get('DWH', 'DWH_USER_PASSWORD')\r\n",
    "DWH_NUMBER_0F_NODES = int(config.get('DWH', 'DWH_NUMBER_0F_NODES'))\r\n",
    "DWH_PORT = int(config.get('DWH', 'DWH_PORT'))\r\n",
    "\r\n",
    "variables = pd.DataFrame({\r\n",
    "    'keys':['DWH_ROLE_NAME', 'DWH_DB_NAME', 'DWH_CLUSTER_ID', 'DWH_NODE_TYPE', 'DWH_NUMBER_0F_NODES', 'DWH_PORT'], \r\n",
    "    'values':[DWH_ROLE_NAME, DWH_DB_NAME, DWH_CLUSTER_ID, DWH_NODE_TYPE, DWH_NUMBER_0F_NODES, DWH_PORT]\r\n",
    "})\r\n",
    "\r\n",
    "variables"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  keys                values\n",
       "0        DWH_ROLE_NAME  redshift_s3_readonly\n",
       "1          DWH_DB_NAME            sparkifydb\n",
       "2       DWH_CLUSTER_ID      sparkify-cluster\n",
       "3        DWH_NODE_TYPE             dc2.large\n",
       "4  DWH_NUMBER_0F_NODES                     4\n",
       "5             DWH_PORT                  5439"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keys</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_ROLE_NAME</td>\n",
       "      <td>redshift_s3_readonly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_DB_NAME</td>\n",
       "      <td>sparkifydb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_CLUSTER_ID</td>\n",
       "      <td>sparkify-cluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_NUMBER_0F_NODES</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create IAM role for Redshift cluster\n",
    "This role will grant redshift AmazonS3ReadOnlyAccess"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Instantiating IAM client\r\n",
    "iam = boto3.client('iam', region_name='us-east-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)\r\n",
    "\r\n",
    "try:\r\n",
    "    print('Creating IAM role for Redshift cluster...')\r\n",
    "    iam_role = iam.create_role(\r\n",
    "        RoleName=DWH_ROLE_NAME,\r\n",
    "        AssumeRolePolicyDocument=json.dumps({\r\n",
    "            'Statement': [{\r\n",
    "                'Action': 'sts:AssumeRole',\r\n",
    "                'Effect': 'Allow',\r\n",
    "                'Principal': {'Service': 'redshift.amazonaws.com'}\r\n",
    "            }],\r\n",
    "            'Version': '2012-10-17'\r\n",
    "        }),\r\n",
    "        Description='Allows Redshift cluster to call AWS services on you behalf',\r\n",
    "    )\r\n",
    "    print('Role creation successful!')\r\n",
    "    \r\n",
    "    \r\n",
    "    iam.attach_role_policy(\r\n",
    "        RoleName=DWH_ROLE_NAME,\r\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess'\r\n",
    "    )['ResponseMetadata']['HTTPStatusCode']\r\n",
    "    \r\n",
    "    print('Role policy attached successfully!')\r\n",
    "except Exception as e:\r\n",
    "    if e.response['Error']['Code'] == 'EntityAlreadyExists':\r\n",
    "        iam_role = iam.get_role(RoleName=DWH_ROLE_NAME)\r\n",
    "        print('Role gotten')\r\n",
    "    else:\r\n",
    "        print(e)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating IAM role for Redshift cluster...\n",
      "Role gotten\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "role_arn = iam_role['Role']['Arn']\r\n",
    "role_arn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build the Redshift cluster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Instantiating redshift client\r\n",
    "redshift = boto3.client('redshift', region_name='us-east-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)\r\n",
    "\r\n",
    "try:\r\n",
    "    print('Creating Redshift cluster...')\r\n",
    "    redshift_cluster = redshift.create_cluster(\r\n",
    "        DBName=DWH_DB_NAME,\r\n",
    "        ClusterIdentifier=DWH_CLUSTER_ID,\r\n",
    "        NodeType=DWH_NODE_TYPE,\r\n",
    "        MasterUsername=DWH_USER_NAME,\r\n",
    "        MasterUserPassword=DWH_USER_PASSWORD,\r\n",
    "        NumberOfNodes=DWH_NUMBER_0F_NODES,\r\n",
    "        IamRoles=[\r\n",
    "            role_arn,\r\n",
    "        ]\r\n",
    "    )\r\n",
    "    print('Redshift cluster creation successful!')\r\n",
    "except Exception as e:\r\n",
    "    print(e)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating Redshift cluster...\n",
      "Redshift cluster creation successful!\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Checking cluster availability status\r\n",
    "cluster_props = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_ID)['Clusters'][0]\r\n",
    "cluster_props['ClusterAvailabilityStatus'], cluster_props['ClusterStatus']"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Obtaining cluster endpoint\r\n",
    "DWH_ENDPOINT = cluster_props['Endpoint']['Address']\r\n",
    "DWH_PORT = int(cluster_props['Endpoint']['Port'])\r\n",
    "print('Endpoint: {}\\nPort: {}'.format(DWH_ENDPOINT, DWH_PORT))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# cluster_vars = pd.DataFrame({\r\n",
    "#     'keys':['ClusterIdentifier', 'NodeType', 'ClusterStatus', 'Endpoint:Address', 'Endpoint:Port', 'IamRole', 'Vpc', 'NumberOfNodes'], \r\n",
    "#     'values':[cluster_props['ClusterIdentifier'], cluster_props['NodeType'], cluster_props['ClusterStatus'], \r\n",
    "#               cluster_props['Endpoint']['Address'], cluster_props['Endpoint']['Port'], cluster_props['IamRoles'][0]['IamRoleArn'],\r\n",
    "#               cluster_props['VpcId'], cluster_props['NumberOfNodes']]\r\n",
    "# })\r\n",
    "\r\n",
    "# cluster_vars"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Open Incomming TCP port to access the cluster endpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# get an ec2 resourse\r\n",
    "ec2 = boto3.resource('ec2', region_name='us-east-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "try: \r\n",
    "    vpc = ec2.Vpc(id=cluster_props['VpcId'])\r\n",
    "    default_sg = list(vpc.security_groups.all())[0]\r\n",
    "    print(default_sg)\r\n",
    "    \r\n",
    "    default_sg.authorize_ingress(\r\n",
    "        GroupName=default_sg.group_name,\r\n",
    "        CidrIp='0.0.0.0/0',\r\n",
    "        IpProtocol='TCP',\r\n",
    "        FromPort=DWH_PORT,\r\n",
    "        ToPort=DWH_PORT\r\n",
    "    )\r\n",
    "except Exception as e:\r\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ETL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%load_ext sql"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "conn_string = 'postgresql://{}:{}@{}:{}/{}'.format(DWH_USER_NAME, DWH_USER_PASSWORD, DWH_ENDPOINT, DWH_PORT, DWH_DB_NAME)\r\n",
    "%sql $conn_string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "%%sql\r\n",
    "DROP TABLE IF EXISTS \"staging_events\";\r\n",
    "DROP TABLE IF EXISTS \"staging_songs\";"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "%%sql\r\n",
    "DROP TABLE IF EXISTS \"staging_events\";\r\n",
    "CREATE TABLE IF NOT EXISTS \"staging_events\" (\r\n",
    "    \"artist\" VARCHAR,\r\n",
    "    \"auth\" VARCHAR,\r\n",
    "    \"firstName\" VARCHAR,\r\n",
    "    \"gender\" VARCHAR,\r\n",
    "    \"itemInSession\" SMALLINT,\r\n",
    "    \"lastName\" VARCHAR,\r\n",
    "    \"length\" REAL,\r\n",
    "    \"level\" VARCHAR,\r\n",
    "    \"location\" VARCHAR,\r\n",
    "    \"method\" VARCHAR,\r\n",
    "    \"page\" VARCHAR,\r\n",
    "    \"registration\" DOUBLE PRECISION,\r\n",
    "    \"sessionId\" SMALLINT,\r\n",
    "    \"song\" VARCHAR,\r\n",
    "    \"status\" SMALLINT,\r\n",
    "    \"ts\" BIGINT,\r\n",
    "    \"userAgent\" VARCHAR,\r\n",
    "    \"userId\" SMALLINT\r\n",
    ");"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "%%sql\r\n",
    "COPY \"staging_events\" FROM 's3://udacity-dend/log-data'\r\n",
    "CREDENTIALS 'aws_iam_role=arn:aws:iam::451737047229:role/redshift_s3_readonly'\r\n",
    "COMPUPDATE OFF REGION 'us-west-2'\r\n",
    "JSON 's3://udacity-dend/log_json_path.json';"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "Done.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "%%sql\r\n",
    "DROP TABLE IF EXISTS \"staging_songs\";\r\n",
    "CREATE TABLE IF NOT EXISTS \"staging_songs\" (\r\n",
    "    \"artist_id\" VARCHAR,\r\n",
    "    \"artist_latitude\" DECIMAL(18,12),\r\n",
    "    \"artist_location\" VARCHAR(MAX),\r\n",
    "    \"artist_longitude\" DECIMAL(18,12),\r\n",
    "    \"artist_name\" VARCHAR(MAX),\r\n",
    "    \"duration\" DOUBLE PRECISION, \r\n",
    "    \"num_songs\" SMALLINT,\r\n",
    "    \"song_id\" VARCHAR,\r\n",
    "    \"title\" VARCHAR(MAX),\r\n",
    "    \"year\" SMALLINT\r\n",
    ");"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "%%sql\r\n",
    "COPY \"staging_songs\" FROM 's3://udacity-dend/song-data'\r\n",
    "CREDENTIALS 'aws_iam_role=arn:aws:iam::451737047229:role/redshift_s3_readonly'\r\n",
    "COMPUPDATE OFF REGION 'us-west-2'\r\n",
    "JSON 'auto ignorecase';"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "%%sql\r\n",
    "SELECT * \r\n",
    "FROM staging_songs_2\r\n",
    "LIMIT 10;"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "0 rows affected.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ],
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>artist_id</th>\n",
       "            <th>artist_latitude</th>\n",
       "            <th>artist_location</th>\n",
       "            <th>artist_longitude</th>\n",
       "            <th>artist_name</th>\n",
       "            <th>duration</th>\n",
       "            <th>num_songs</th>\n",
       "            <th>song_id</th>\n",
       "            <th>title</th>\n",
       "            <th>year</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "%%sql\r\n",
    "DROP TABLE IF EXISTS \"staging_songs_2\";\r\n",
    "CREATE TABLE IF NOT EXISTS \"staging_songs_2\" (\r\n",
    "    \"artist_id\" VARCHAR,\r\n",
    "    \"artist_latitude\" DECIMAL(18,12),\r\n",
    "    \"artist_location\" VARCHAR(MAX),\r\n",
    "    \"artist_longitude\" DECIMAL(18,12),\r\n",
    "    \"artist_name\" VARCHAR(MAX),\r\n",
    "    \"duration\" DOUBLE PRECISION, \r\n",
    "    \"num_songs\" SMALLINT,\r\n",
    "    \"song_id\" VARCHAR,\r\n",
    "    \"title\" VARCHAR(MAX),\r\n",
    "    \"year\" SMALLINT\r\n",
    ");"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "%%sql\r\n",
    "COPY \"staging_songs_2\" FROM 's3://udacity-dend/song_data'\r\n",
    "CREDENTIALS 'aws_iam_role=arn:aws:iam::451737047229:role/redshift_s3_readonly'\r\n",
    "COMPUPDATE OFF REGION 'us-west-2'\r\n",
    "JSON 'auto ignorecase'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "Done.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "%%sql\r\n",
    "WITH main_table AS (\r\n",
    "    SELECT *\r\n",
    "    FROM staging_events\r\n",
    "    WHERE page = 'NextSong'\r\n",
    "), timestamp_table AS (\r\n",
    "    SELECT ts, timestamp 'epoch' + (ts/1000) * interval '1 second' AS t_stamp\r\n",
    "    FROM main_table\r\n",
    ")\r\n",
    "\r\n",
    "SELECT t_stamp, date_part('weekday', t_stamp) AS weekday\r\n",
    "FROM timestamp_table\r\n",
    "ORDER BY 1 DESC\r\n",
    "LIMIT 10;"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "10 rows affected.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(datetime.datetime(2018, 11, 30, 19, 54, 24), 5.0),\n",
       " (datetime.datetime(2018, 11, 30, 18, 51, 24), 5.0),\n",
       " (datetime.datetime(2018, 11, 30, 18, 47, 58), 5.0),\n",
       " (datetime.datetime(2018, 11, 30, 18, 44, 36), 5.0),\n",
       " (datetime.datetime(2018, 11, 30, 18, 40, 5), 5.0),\n",
       " (datetime.datetime(2018, 11, 30, 18, 39, 22), 5.0),\n",
       " (datetime.datetime(2018, 11, 30, 18, 36, 25), 5.0),\n",
       " (datetime.datetime(2018, 11, 30, 18, 35, 19), 5.0),\n",
       " (datetime.datetime(2018, 11, 30, 18, 32, 46), 5.0),\n",
       " (datetime.datetime(2018, 11, 30, 18, 31, 34), 5.0)]"
      ],
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>t_stamp</th>\n",
       "            <th>weekday</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2018-11-30 19:54:24</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2018-11-30 18:51:24</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2018-11-30 18:47:58</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2018-11-30 18:44:36</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2018-11-30 18:40:05</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2018-11-30 18:39:22</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2018-11-30 18:36:25</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2018-11-30 18:35:19</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2018-11-30 18:32:46</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2018-11-30 18:31:34</td>\n",
       "            <td>5.0</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "execution_count": 186
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "source": [
    "%%sql\r\n",
    "DROP TABLE IF EXISTS \"time\";\r\n",
    "CREATE TABLE IF NOT EXISTS \"time\" (\r\n",
    "    start_time BIGINT NOT NULL SORTKEY,\r\n",
    "    hour SMALLINT NOT NULL,\r\n",
    "    day SMALLINT NOT NULL,\r\n",
    "    week SMALLINT NOT NULL,\r\n",
    "    month SMALLINT NOT NULL,\r\n",
    "    year SMALLINT NOT NULL,\r\n",
    "    weekday SMALLINT NOT NULL\r\n",
    ")\r\n",
    "DISTSTYLE ALL;"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 272
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "source": [
    "%%sql\r\n",
    "INSERT INTO \"time\" (\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\")\r\n",
    "SELECT DISTINCT ts AS start_time,\r\n",
    "        DATE_PART('hour', TIMESTAMP 'epoch' + (ts/1000) * INTERVAL '1 second') AS hour,\r\n",
    "        DATE_PART('day', TIMESTAMP 'epoch' + (ts/1000) * INTERVAL '1 second') AS day,\r\n",
    "        DATE_PART('week', TIMESTAMP 'epoch' + (ts/1000) * INTERVAL '1 second') AS week,\r\n",
    "        DATE_PART('month', TIMESTAMP 'epoch' + (ts/1000) * INTERVAL '1 second') AS month,\r\n",
    "        DATE_PART('year', TIMESTAMP 'epoch' + (ts/1000) * INTERVAL '1 second') AS year,\r\n",
    "        DATE_PART('weekday', TIMESTAMP 'epoch' + (ts/1000) * INTERVAL '1 second') AS weekday\r\n",
    "FROM staging_events;"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "8023 rows affected.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 274
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "source": [
    "%%sql\r\n",
    "SELECT *\r\n",
    "FROM time\r\n",
    "ORDER BY start_time DESC\r\n",
    "LIMIT 10;"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * postgresql://mike:***@sparkify-cluster.csc0efk6rxxc.us-east-2.redshift.amazonaws.com:5439/sparkifydb\n",
      "10 rows affected.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(1543607664796, 19, 30, 48, 11, 2018, 5),\n",
       " (1543603993796, 18, 30, 48, 11, 2018, 5),\n",
       " (1543603884796, 18, 30, 48, 11, 2018, 5),\n",
       " (1543603678796, 18, 30, 48, 11, 2018, 5),\n",
       " (1543603476796, 18, 30, 48, 11, 2018, 5),\n",
       " (1543603205796, 18, 30, 48, 11, 2018, 5),\n",
       " (1543603162796, 18, 30, 48, 11, 2018, 5),\n",
       " (1543602985796, 18, 30, 48, 11, 2018, 5),\n",
       " (1543602936796, 18, 30, 48, 11, 2018, 5),\n",
       " (1543602919796, 18, 30, 48, 11, 2018, 5)]"
      ],
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>start_time</th>\n",
       "            <th>hour</th>\n",
       "            <th>day</th>\n",
       "            <th>week</th>\n",
       "            <th>month</th>\n",
       "            <th>year</th>\n",
       "            <th>weekday</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1543607664796</td>\n",
       "            <td>19</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1543603993796</td>\n",
       "            <td>18</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1543603884796</td>\n",
       "            <td>18</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1543603678796</td>\n",
       "            <td>18</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1543603476796</td>\n",
       "            <td>18</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1543603205796</td>\n",
       "            <td>18</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1543603162796</td>\n",
       "            <td>18</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1543602985796</td>\n",
       "            <td>18</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1543602936796</td>\n",
       "            <td>18</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1543602919796</td>\n",
       "            <td>18</td>\n",
       "            <td>30</td>\n",
       "            <td>48</td>\n",
       "            <td>11</td>\n",
       "            <td>2018</td>\n",
       "            <td>5</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "execution_count": 276
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "s3 = boto3.resource('s3', aws_access_key_id=KEY, aws_secret_access_key=SECRET)\r\n",
    "u_bucker = s3.Bucket('udacity-dend')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "count = 0\r\n",
    "total_size = 0\r\n",
    "for obj in u_bucker.objects.filter(Prefix='song-data'):\r\n",
    "    count += 1\r\n",
    "    total_size += obj.size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "count"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "385253"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "total_size"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "103473867"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cleaning up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "source": [
    "# response = redshift.delete_cluster(ClusterIdentifier=DWH_CLUSTER_ID, SkipFinalClusterSnapshot=True)\r\n",
    "# response"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# iam.detach_role_policy(RoleName=DWH_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\r\n",
    "# iam.delete_role(RoleName=DWH_ROLE_NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}